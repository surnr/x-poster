# X-Poster: Automated AI-Powered Tweet Generator

An intelligent, modular Cloudflare Workers application that automatically generates and posts engaging tweets from real-time news sources using AI. Built with a plugin-based architecture that makes adding new data sources trivial.

X-Poster solves a critical challenge: **LLMs are not real-time aware**. While models like GPT-5.1 have knowledge cutoffs, X-Poster bridges this gap by:

1. **Live Context Engineering** â€” Scrapes fresh content from real-time news sources (HackerNews, TechCrunch, Yahoo Finance).
2. **Dynamic Prompt Construction** â€” Injects the most recent article content directly into AI prompts.
3. **Source-Specific Personas** â€” Applies tailored AI instructions per source to match voice and context.
4. **Automatic Posting** â€” Produces human-like tweets and posts them to Twitter/X every 4 hours.

This approach guarantees your tweets reflect **current events** and **breaking news**, not outdated model training data.

## Workflow Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cloudflare Worker (Edge)                         â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Cron Trigger (Every 4h)                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â”‚                                             â”‚
â”‚                       â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Rotation Service (rotation.ts)                  â”‚   â”‚
â”‚  â”‚   Selects data source: Math.floor(hour/4) % sources.length   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â”‚                                             â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚         â”‚             â”‚             â”‚                               â”‚
â”‚         â–¼             â–¼             â–¼                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚HackerNewsâ”‚  â”‚TechCrunchâ”‚  â”‚  Yahoo   â”‚  (Plugin Architecture)    â”‚
â”‚  â”‚  .ts     â”‚  â”‚  .ts     â”‚  â”‚Finance.tsâ”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                           â”‚
â”‚        â”‚             â”‚             â”‚                                â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                      â”‚                                              â”‚
â”‚                      â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 HTML Parser (html-parser.ts)                 â”‚   â”‚
â”‚  â”‚  Extracts: Title, Content, Links using CSS selectors         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â”‚                                             â”‚
â”‚                       â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           Content Processor (content-processor.ts)           â”‚   â”‚
â”‚  â”‚  Cleans, truncates (30k chars), validates content            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â”‚                                             â”‚
â”‚                       â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚            AI Gateway Service (ai-gateway.ts)                â”‚   â”‚
â”‚  â”‚  Constructs: System Prompt + User Message (article content)  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Cloudflare AI Gatewayâ”‚
              â”‚    (Rate Limiting,   â”‚
              â”‚   Caching, Analytics)â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   OpenAI API    â”‚
                â”‚   (GPT-4 Turbo) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Generated Tweet â”‚
                â”‚  (255-280 chars) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Twitter Client      â”‚
              â”‚ (twitter.ts)        â”‚
              â”‚ OAuth 1.0a Auth     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Twitter/X   â”‚
                  â”‚   API v2    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Context Engineering Strategy

### The Problem

LLMs are trained on data with a cutoff date. They can't know about:

- Breaking news from today
- Trending GitHub repos this week
- Latest crypto market movements
- New AI breakthroughs announced yesterday

### Our Solution: Real-Time Context Injection

```typescript
// Traditional approach (outdated):
"Write a tweet about AI news" â†’ LLM â†’ Tweet (based on old training data)

// X-Poster approach (real-time):
1. Scrape TechCrunch AI category (right now)
2. Extract latest article: "OpenAI releases GPT-5 with video understanding"
3. Build prompt: "Title: GPT-5 Released\nContent: [full article text]"
4. Send to LLM â†’ Gets tweet about TODAY's news
```

### How Each Data Source Works

**HackerNews** (`hackernews.ts`)

```
1. Fetch: https://news.ycombinator.com/news
2. Extract: First trending article link (.titleline a)
3. Scrape: Full article content
4. Context: "You're a tech-savvy developer sharing HN finds..."
5. Result: Tweet about latest trending tech discussion
```

**TechCrunch** (`techcrunch.ts`)

```
1. Fetch: https://techcrunch.com/category/artificial-intelligence/
2. Extract: Latest AI news article (.loop-card__title-link)
3. Scrape: Article title + body (.entry-content)
4. Context: "You're a news-savvy enthusiast sharing AI breakthroughs..."
5. Result: Tweet about breaking AI/startup news
```

**Yahoo Finance** (`yahoo-finance.ts`)

```
1. Fetch: https://finance.yahoo.com/topic/crypto/
2. Extract: Latest crypto news (.titles)
3. Scrape: Article title + content (.cover-title, .body)
4. Context: "You're a Web3 founder who trades crypto..."
5. Result: Tweet about current crypto market events
```

## Getting Started

### Prerequisites

- Node.js 18+
- Cloudflare account
- Twitter Developer account
- OpenAI API key

### Local Setup & Testing

#### 1. Clone the Repository

```bash
git clone https://github.com/surnr/x-poster.git
cd x-poster
```

#### 2. Install Dependencies

```bash
npm install
```

#### 3. Configure Local .env

Copy the example env file and update values locally:

```bash
cp .env.example .env
# Open .env and fill in your credentials:
# - OPENAI_API_KEY
# - AI_GATEWAY_ACCOUNT_ID
# - AI_GATEWAY_NAME
# - TWITTER_API_KEY
# - TWITTER_API_SECRET
# - TWITTER_ACCESS_TOKEN
# - TWITTER_ACCESS_SECRET
```

#### 4. Start Local Development Server

```bash
npm run dev
```

The worker will start at `http://localhost:8787`

### Testing the Workflow

#### Test Endpoints

**Check Status**

```bash
curl http://localhost:8787/status
```

Returns current rotation status, next source, and countdown.

**View Schedule**

```bash
curl http://localhost:8787/schedule
```

See upcoming tweet schedule for next 24 hours.

**Manual Trigger (HTTP)**

```bash
curl http://localhost:8787/trigger
```

Manually runs the full workflow without waiting for cron.

#### **Test Scheduled Workflow Locally** âš¡

To test the actual cron job logic locally:

```bash
curl http://localhost:8787/__scheduled?cron=*+*+*+*+*
```

Or use the Cloudflare-specific handler URL:

```bash
curl "http://localhost:8787/cdn-cgi/mf/scheduled"
```

This simulates the scheduled trigger and runs the complete workflow:

1. Selects data source based on current time
2. Scrapes listing page
3. Extracts article content
4. Generates tweet with AI
5. Posts to Twitter (if credentials configured)

**Watch the logs** in your terminal to see the full execution flow!

### ğŸ“ Project Structure

```
x-poster/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ types/              # TypeScript type definitions
â”‚   â”‚   â”œâ”€â”€ datasource.ts   # DataSource interface (plugin contract)
â”‚   â”‚   â”œâ”€â”€ env.ts          # Environment bindings
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”œâ”€â”€ utils/              # Shared utilities
â”‚   â”‚   â”œâ”€â”€ html-parser.ts  # CSS selector-based extraction
â”‚   â”‚   â”œâ”€â”€ content-processor.ts  # Text cleaning & truncation
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”œâ”€â”€ datasources/        # Pluggable data sources
â”‚   â”‚   â”œâ”€â”€ hackernews.ts   # HackerNews scraper
â”‚   â”‚   â”œâ”€â”€ techcrunch.ts   # TechCrunch scraper
â”‚   â”‚   â”œâ”€â”€ yahoo-finance.ts  # Yahoo Finance scraper
â”‚   â”‚   â”œâ”€â”€ _template.ts    # Template for new sources
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”œâ”€â”€ services/           # Core business logic
â”‚   â”‚   â”œâ”€â”€ rotation.ts     # Time-based source selection
â”‚   â”‚   â”œâ”€â”€ ai-gateway.ts   # OpenAI integration
â”‚   â”‚   â”œâ”€â”€ twitter.ts      # Twitter API client
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”œâ”€â”€ config/             # Configuration
â”‚   â”‚   â””â”€â”€ datasources.ts  # Source registry (add sources here)
â”‚   â””â”€â”€ index.ts            # Main worker entry point
â”œâ”€â”€ wrangler.jsonc          # Cloudflare Workers config
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ DATASOURCE_EXAMPLES.md  # Examples for new sources
â””â”€â”€ README.md
```

## Adding New Data Sources

The plugin architecture makes adding sources incredibly easy:

### 1. Create Source File

```bash
cp src/datasources/_template.ts src/datasources/reddit.ts
```

### 2. Implement Interface

```typescript
export class Reddit implements DataSource {
  getName(): string {
    return "Reddit";
  }

  getListingUrl(): string {
    return "https://www.reddit.com/r/technology/hot.json";
  }

  async scrapeArticleLinks(html: string): Promise<string[]> {
    const data = JSON.parse(html);
    return data.data.children.map((post) => post.data.url);
  }

  async fetchArticleContent(url: string): Promise<ScrapedArticle> {
    // Fetch and extract content
  }

  getSystemPrompt(): string {
    return "You discover cool Reddit posts...";
  }

  getUserMessage(article: ScrapedArticle): string {
    return `Title: ${article.title}\n\nContent: ${article.content}`;
  }

  getCharacterLimit(): number {
    return 280;
  }
}
```

### 3. Register Source

Edit `src/config/datasources.ts`:

```typescript
export const DATA_SOURCES: DataSource[] = [
  new HackerNews(),
  new TechCrunch(),
  new YahooFinance(),
  new Reddit(), // â† Add here
];
```

That's it! Your new source is now in the rotation.

See `DATASOURCE_EXAMPLES.md` for complete examples (Reddit, Product Hunt, Dev.to, Medium, GitHub).

## ğŸ”„ How Rotation Works

**Formula**: `Math.floor(currentHour / 4) % numberOfSources`

With 3 sources and 4-hour intervals:

```
00:00-03:59 â†’ HackerNews
04:00-07:59 â†’ TechCrunch
08:00-11:59 â†’ Yahoo Finance
12:00-15:59 â†’ HackerNews
16:00-19:59 â†’ TechCrunch
20:00-23:59 â†’ Yahoo Finance
```

Add more sources and rotation automatically adjusts:

- 4 sources = changes every 6 hours
- 6 sources = changes every 4 hours
- 8 sources = changes every 3 hours

## Deployment

```bash
npm run deploy
```

Your worker will be live at `https://x-poster.your-subdomain.workers.dev`

The cron job runs automatically every 4 hours.

---

**Made with â¤ï¸ for the real-time AI community**
